from pythainlp.tokenize import word_tokenize

class Tokenization:
    def __init__(self, text):
        pass

    def tokenize(text):
        # print(word_tokenize(text, keep_whitespace=False))
        return word_tokenize(text, keep_whitespace=False)
